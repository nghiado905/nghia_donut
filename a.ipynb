{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# from collections import defaultdict\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def parse_cord_v2_json(json_path):\n",
    "#     with open(json_path, 'r', encoding='utf-8') as f:\n",
    "#         data = json.load(f)\n",
    "\n",
    "#     grouped = defaultdict(lambda: {\"count\": \"\", \"name\": \"\", \"price\": \"\"})\n",
    "\n",
    "#     total_price = \"\"\n",
    "    \n",
    "#     for line in data[\"valid_line\"]:\n",
    "#         category = line[\"category\"]\n",
    "#         group_id = line[\"group_id\"]\n",
    "#         words = \" \".join(w[\"text\"] for w in line[\"words\"])\n",
    "\n",
    "#         if category == \"menu.cnt\":\n",
    "#             grouped[group_id][\"count\"] = words\n",
    "#         elif category in [\"menu.nm\", \"menu.sub_nm\"]:\n",
    "#             grouped[group_id][\"name\"] += words + \" \"\n",
    "#         elif category == \"menu.price\":\n",
    "#             grouped[group_id][\"price\"] = words\n",
    "#         elif category == \"total.total_price\":\n",
    "#             total_price = words\n",
    "\n",
    "#     items = []\n",
    "#     for g in sorted(grouped.keys()):\n",
    "#         entry = grouped[g]\n",
    "#         name = entry[\"name\"].strip()\n",
    "#         if name and entry[\"price\"]:\n",
    "#             items.append({\n",
    "#                 \"name\": name,\n",
    "#                 \"count\": entry[\"count\"],\n",
    "#                 \"price\": entry[\"price\"]\n",
    "#             })\n",
    "\n",
    "#     result = {\n",
    "#         \"items\": items,\n",
    "#         \"total\": total_price\n",
    "#     }\n",
    "\n",
    "#     return result\n",
    "\n",
    "# def prepare_donut_cord_dataset(json_dir, image_dir, output_path):\n",
    "#     dataset = []\n",
    "\n",
    "#     json_files = [f for f in os.listdir(json_dir) if f.endswith(\".json\")]\n",
    "\n",
    "#     for json_file in tqdm(json_files, desc=\"Processing JSON files\"):\n",
    "#         json_path = os.path.join(json_dir, json_file)\n",
    "#         image_name = os.path.splitext(json_file)[0] + \".png\"  # n·∫øu l√† .jpg s·ª≠a ·ªü ƒë√¢y\n",
    "#         image_path = os.path.join(image_dir, image_name)\n",
    "\n",
    "#         if not os.path.exists(image_path):\n",
    "#             print(f\"Image not found for {json_file}, skipping...\")\n",
    "#             continue\n",
    "\n",
    "#         text_output = parse_cord_v2_json(json_path)\n",
    "        \n",
    "#         dataset.append({\n",
    "#             \"id\": json_file.replace(\".json\", \"\"),\n",
    "#             \"image_path\": image_path,\n",
    "#             \"text_input\": \"<s_cord-v2>\",  # prefix task\n",
    "#             \"text_output\": json.dumps(text_output, ensure_ascii=False)\n",
    "#         })\n",
    "\n",
    "#     with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#         json.dump(dataset, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "#     print(f\"‚úÖ Done! Saved {len(dataset)} samples to {output_path}\")\n",
    "\n",
    "# # C√°ch ch·∫°y\n",
    "# prepare_donut_cord_dataset(\n",
    "#     json_dir=\"data/json\",\n",
    "#     image_dir=\"data/image\",\n",
    "#     output_path=\"data/train.json\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# image_dir = \"data/images\"\n",
    "# annotation_dir = \"data/annotations\"\n",
    "\n",
    "# # Ki·ªÉm tra th∆∞ m·ª•c t·ªìn t·∫°i\n",
    "# print(\"Th∆∞ m·ª•c images t·ªìn t·∫°i:\", os.path.exists(image_dir))\n",
    "# print(\"Th∆∞ m·ª•c annotation t·ªìn t·∫°i:\", os.path.exists(annotation_dir))\n",
    "\n",
    "# # Li·ªát k√™ c√°c file trong th∆∞ m·ª•c\n",
    "# print(\"File trong images:\", os.listdir(image_dir))\n",
    "# print(\"File trong annotation:\", os.listdir(annotation_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# import shutil\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # ƒê·ªãnh nghƒ©a th∆∞ m·ª•c g·ªëc v√† t·ª∑ l·ªá chia d·ªØ li·ªáu\n",
    "# dataset_name = \"cord_dataset\"\n",
    "# image_dir = \"data/images\"  # Thay b·∫±ng ƒë∆∞·ªùng d·∫´n th·ª±c t·∫ø n·∫øu c·∫ßn\n",
    "# annotation_dir = \"data/annotations\"  # Thay b·∫±ng ƒë∆∞·ªùng d·∫´n th·ª±c t·∫ø n·∫øu c·∫ßn\n",
    "# train_ratio, val_ratio, test_ratio = 0.8, 0.1, 0.1\n",
    "\n",
    "# # Ki·ªÉm tra th∆∞ m·ª•c\n",
    "# if not os.path.exists(image_dir):\n",
    "#     raise FileNotFoundError(f\"Th∆∞ m·ª•c {image_dir} kh√¥ng t·ªìn t·∫°i\")\n",
    "# if not os.path.exists(annotation_dir):\n",
    "#     raise FileNotFoundError(f\"Th∆∞ m·ª•c {annotation_dir} kh√¥ng t·ªìn t·∫°i\")\n",
    "\n",
    "# # T·∫°o th∆∞ m·ª•c cho dataset\n",
    "# os.makedirs(dataset_name, exist_ok=True)\n",
    "# for split in [\"train\", \"validation\", \"test\"]:\n",
    "#     os.makedirs(os.path.join(dataset_name, split), exist_ok=True)\n",
    "\n",
    "# # H√†m x·ª≠ l√Ω file JSON ƒë·ªÉ t·∫°o ground truth\n",
    "# def process_cord_json(json_path):\n",
    "#     with open(json_path, 'r') as f:\n",
    "#         data = json.load(f)\n",
    "\n",
    "#     menu_items = []\n",
    "#     subtotal = None\n",
    "#     total_info = {}\n",
    "\n",
    "#     grouped_lines = {}\n",
    "#     for line in data[\"valid_line\"]:\n",
    "#         group_id = line[\"group_id\"]\n",
    "#         if group_id not in grouped_lines:\n",
    "#             grouped_lines[group_id] = []\n",
    "#         grouped_lines[group_id].append(line)\n",
    "\n",
    "#     for group_id, lines in grouped_lines.items():\n",
    "#         menu_item = {}\n",
    "#         for line in lines:\n",
    "#             category = line[\"category\"]\n",
    "#             text = \" \".join(word[\"text\"] for word in line[\"words\"])\n",
    "\n",
    "#             if category.startswith(\"menu\"):\n",
    "#                 if category == \"menu.nm\":\n",
    "#                     menu_item[\"name\"] = text\n",
    "#                 elif category == \"menu.cnt\":\n",
    "#                     menu_item[\"count\"] = text\n",
    "#                 elif category == \"menu.price\":\n",
    "#                     menu_item[\"price\"] = text\n",
    "#             elif category == \"sub_total.subtotal_price\":\n",
    "#                 subtotal = text\n",
    "#             elif category.startswith(\"total\"):\n",
    "#                 if category == \"total.total_price\":\n",
    "#                     total_info[\"total_price\"] = text\n",
    "#                 elif category == \"total.cashprice\":\n",
    "#                     total_info[\"cash\"] = text\n",
    "#                 elif category == \"total.changeprice\":\n",
    "#                     total_info[\"change\"] = text\n",
    "#                 elif category == \"total.menutype_cnt\":\n",
    "#                     total_info[\"menu_types\"] = text\n",
    "#                 elif category == \"total.menuqty_cnt\":\n",
    "#                     total_info[\"menu_quantity\"] = text\n",
    "\n",
    "#         if menu_item:\n",
    "#             menu_item.setdefault(\"count\", None)\n",
    "#             menu_item.setdefault(\"price\", None)\n",
    "#             menu_items.append(menu_item)\n",
    "\n",
    "#     ground_truth = {\n",
    "#         \"menu\": menu_items,\n",
    "#         \"subtotal\": subtotal,\n",
    "#         \"total\": total_info\n",
    "#     }\n",
    "#     return ground_truth\n",
    "\n",
    "# # Thu th·∫≠p t·∫•t c·∫£ c√°c file JSON v√† ·∫£nh\n",
    "# data = []\n",
    "# for json_file in os.listdir(annotation_dir):\n",
    "#     if json_file.endswith(\".json\"):\n",
    "#         image_file_base = json_file.replace(\".json\", \"\")\n",
    "#         image_path = None\n",
    "#         for ext in [\".jpg\", \".png\"]:  # H·ªó tr·ª£ nhi·ªÅu ƒë·ªãnh d·∫°ng ·∫£nh\n",
    "#             temp_path = os.path.join(image_dir, image_file_base + ext)\n",
    "#             if os.path.exists(temp_path):\n",
    "#                 image_path = temp_path\n",
    "#                 image_file = image_file_base + ext\n",
    "#                 break\n",
    "\n",
    "#         if image_path is None:\n",
    "#             print(f\"Kh√¥ng t√¨m th·∫•y ·∫£nh cho {json_file}\")\n",
    "#             continue\n",
    "\n",
    "#         json_path = os.path.join(annotation_dir, json_file)\n",
    "#         ground_truth = process_cord_json(json_path)\n",
    "#         data.append({\n",
    "#             \"image_file\": image_file,\n",
    "#             \"image_path\": image_path,\n",
    "#             \"ground_truth\": ground_truth\n",
    "#         })\n",
    "\n",
    "# # Ki·ªÉm tra xem c√≥ d·ªØ li·ªáu kh√¥ng\n",
    "# if not data:\n",
    "#     raise ValueError(\"Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu n√†o ƒë·ªÉ x·ª≠ l√Ω. Vui l√≤ng ki·ªÉm tra th∆∞ m·ª•c images v√† annotation.\")\n",
    "\n",
    "# # Chia d·ªØ li·ªáu th√†nh train, validation, test\n",
    "# train_data, temp_data = train_test_split(data, train_size=train_ratio, random_state=42)\n",
    "# val_data, test_data = train_test_split(temp_data, test_size=test_ratio/(test_ratio + val_ratio), random_state=42)\n",
    "\n",
    "# # H√†m l∆∞u d·ªØ li·ªáu v√†o th∆∞ m·ª•c v√† t·∫°o metadata.jsonl\n",
    "# def save_split(split_data, split_name):\n",
    "#     split_dir = os.path.join(dataset_name, split_name)\n",
    "#     metadata_path = os.path.join(split_dir, \"metadata.jsonl\")\n",
    "\n",
    "#     with open(metadata_path, \"w\") as f:\n",
    "#         for item in split_data:\n",
    "#             dest_image_path = os.path.join(split_dir, item[\"image_file\"])\n",
    "#             shutil.copy(item[\"image_path\"], dest_image_path)\n",
    "\n",
    "#             metadata = {\n",
    "#                 \"file_name\": item[\"image_file\"],\n",
    "#                 \"ground_truth\": json.dumps({\"gt_parse\": item[\"ground_truth\"]})\n",
    "#             }\n",
    "#             f.write(json.dumps(metadata) + \"\\n\")\n",
    "\n",
    "# # L∆∞u d·ªØ li·ªáu v√†o c√°c th∆∞ m·ª•c\n",
    "# save_split(train_data, \"train\")\n",
    "# save_split(val_data, \"validation\")\n",
    "# save_split(test_data, \"test\")\n",
    "\n",
    "# print(\"Dataset ƒë√£ ƒë∆∞·ª£c t·∫°o xong!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python test.py --dataset_name_or_path dataset --pretrained_model_name_or_path result_new/train_cord/test_experiment --save_path ./result/output.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--task\", type=str, default=\"cord-v2\")\n",
    "# parser.add_argument(\"--pretrained_path\", type=str, default=\"naver-clova-ix/donut-base-finetuned-cord-v2\")\n",
    "# args, left_argv = parser.parse_known_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gradio as gr\n",
    "import torch\n",
    "from PIL import Image\n",
    "from donut import DonutModel\n",
    "\n",
    "def demo_process(input_img):\n",
    "    global pretrained_model, task_prompt, task_name\n",
    "    input_img = Image.fromarray(input_img)\n",
    "    output = pretrained_model.inference(image=input_img, prompt=task_prompt)[\"predictions\"][0]\n",
    "    return output\n",
    "\n",
    "def demo_process_vqa(input_img, question):\n",
    "    global pretrained_model, task_prompt, task_name\n",
    "    input_img = Image.fromarray(input_img)\n",
    "    user_prompt = task_prompt.replace(\"{user_input}\", question)\n",
    "    output = pretrained_model.inference(input_img, prompt=user_prompt)[\"predictions\"][0]\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = 'cord'\n",
    "task_prompt = f\"<s_{task_name}>\"\n",
    "\n",
    "# pretrained_model = DonutModel.from_pretrained(r\"result_new/train_cord/test_experiment\")\n",
    "pretrained_model = DonutModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-cord-v2\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    pretrained_model.half()\n",
    "    device = torch.device(\"cuda\")\n",
    "    pretrained_model.to(device)\n",
    "else:\n",
    "    pretrained_model.encoder.to(torch.bfloat16)\n",
    "\n",
    "# pretrained_model.eval()\n",
    "demo = gr.Interface(\n",
    "    fn=demo_process_vqa if task_name == \"docvqa\" else demo_process,\n",
    "    inputs=[\"image\", \"text\"] if task_name == \"docvqa\" else \"image\",\n",
    "    outputs=\"json\",\n",
    "    title=f\"Donut üç© demonstration for `{task_name}` task\",\n",
    ")\n",
    "demo.launch(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = DonutModel.from_pretrained(\"donut-base-finetuned-docvqa\")\n",
    "\n",
    "def demo_process(input_img):\n",
    "    global pretrained_model, task_prompt, task_name\n",
    "    input_img = Image.fromarray(input_img)\n",
    "    output = pretrained_model.inference(image=input_img, prompt=task_prompt)[\"predictions\"][0]\n",
    "    return output\n",
    "\n",
    "image = cv2.imread(r'D:\\Private\\Project\\kaggle\\LabAI\\Donut\\image.png')\n",
    "demo_process(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from donut import DonutModel\n",
    "\n",
    "# Define the demo_process function (as provided)\n",
    "\n",
    "pretrained_model = DonutModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-cord-v2\")\n",
    "\n",
    "def demo_process(input_img):\n",
    "    global pretrained_model, task_prompt, task_name\n",
    "    input_img = Image.fromarray(input_img)\n",
    "    output = pretrained_model.inference(image=input_img, prompt=task_prompt)[\"predictions\"][0]\n",
    "    return output\n",
    "\n",
    "# Directory containing images and output\n",
    "image_dir = r'D:\\Private\\Project\\kaggle\\LabAI\\Donut\\donut\\dataset\\train'\n",
    "output_dir = r'D:\\Private\\Project\\kaggle\\LabAI\\Donut\\donut\\dataset\\output'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get all image files in the directory (e.g., .jpg files)\n",
    "image_paths = glob.glob(os.path.join(image_dir, '*.jpg'))\n",
    "\n",
    "# Process each image\n",
    "for image_path in image_paths:\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Check if the image was loaded successfully\n",
    "    if image is None:\n",
    "        print(f\"Could not load image at {image_path}\")\n",
    "        continue\n",
    "    \n",
    "    # Convert BGR to RGB\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Process the image to get the output\n",
    "    try:\n",
    "        output = demo_process(image_rgb)\n",
    "        # Shorten the output (e.g., first 100 characters)\n",
    "        short_output = str(output)[:100] + ('...' if len(str(output)) > 100 else '')\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Create a figure with a larger image and smaller text area\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Display the input image (larger)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image_rgb)\n",
    "    plt.title('Input Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Display the shortened output text (smaller area)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.text(0.1, 0.5, short_output, fontsize=10, wrap=True, verticalalignment='center')\n",
    "    plt.title('Output')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Adjust layout to make image dominant\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the combined visualization\n",
    "    output_filename = os.path.join(output_dir, f'result_{os.path.basename(image_path)}')\n",
    "    plt.savefig(output_filename, bbox_inches='tight', dpi=150)\n",
    "    print(f\"Saved result for {image_path} to {output_filename}\")\n",
    "    \n",
    "    # Close the plot to free memory\n",
    "    plt.close()\n",
    "\n",
    "print(\"Processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "# from decord import VideoReader, cpu\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "def load_image(image_file, input_size=448, max_num=12):\n",
    "    image = Image.open(image_file).convert('RGB')\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    \"5CD-AI/Vintern-1B-v2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    ").eval().cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"5CD-AI/Vintern-1B-v2\", trust_remote_code=True, use_fast=False)\n",
    "\n",
    "test_image = r'D:\\Private\\Project\\kaggle\\LabAI\\Donut\\donut\\dataset\\train\\page_MSBbank_24.jpg'\n",
    "\n",
    "pixel_values = load_image(test_image, max_num=12).to(torch.bfloat16).cuda()\n",
    "generation_config = dict(max_new_tokens= 1024, do_sample=False, num_beams = 3, repetition_penalty=2.5)\n",
    "\n",
    "question = '<image>\\nM√¥ t·∫£ h√¨nh ·∫£nh m·ªôt c√°ch chi ti·∫øt.'\n",
    "\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "#question = \"C√¢u h·ªèi kh√°c ......\"\n",
    "#response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=history, return_history=True)\n",
    "#print(f'User: {question}\\nAssistant: {response}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import list_repo_files\n",
    "print(list_repo_files(\"5CD-AI/Vintern-1B-v2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
