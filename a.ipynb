{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import os\n",
    "# from PIL import Image\n",
    "# import random\n",
    "\n",
    "# def create_donut_dataset(data_dir, output_dir, dataset_name, split_ratio=0.8):\n",
    "#     \"\"\"\n",
    "#     Convert OCR dataset to Donut dataset format with metadata.jsonl files.\n",
    "#     Assumes images in data_dir/final_data and labels in data_dir/labels.\n",
    "#     \"\"\"\n",
    "#     # Create output directories\n",
    "#     os.makedirs(os.path.join(output_dir, dataset_name, 'train'), exist_ok=True)\n",
    "#     os.makedirs(os.path.join(output_dir, dataset_name, 'validation'), exist_ok=True)\n",
    "#     os.makedirs(os.path.join(output_dir, dataset_name, 'test'), exist_ok=True)\n",
    "\n",
    "#     # Initialize metadata files\n",
    "#     train_metadata = []\n",
    "#     val_metadata = []\n",
    "#     test_metadata = []\n",
    "\n",
    "#     # Paths to images and labels\n",
    "#     image_dir = os.path.join(data_dir, 'final_data')\n",
    "#     label_dir = os.path.join(data_dir, 'labels')\n",
    "\n",
    "#     image_files = [f for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "#     # Shuffle images for random splitting\n",
    "#     random.shuffle(image_files)\n",
    "\n",
    "#     # Calculate split indices\n",
    "#     total_images = len(image_files)\n",
    "#     train_end = int(total_images * split_ratio)\n",
    "#     val_end = int(total_images * (split_ratio + (1-split_ratio)/2))\n",
    "\n",
    "#     for idx, image_file in enumerate(image_files):\n",
    "#         # Get corresponding label file (e.g., img_4.txt for img_4.jpg)\n",
    "#         label_file = os.path.splitext(image_file)[0] + '.txt'\n",
    "#         label_path = os.path.join(label_dir, label_file)\n",
    "\n",
    "#         if not os.path.exists(label_path):\n",
    "#             print(f\"Label file {label_file} not found, skipping {image_file}\")\n",
    "#             continue\n",
    "\n",
    "#         # Read label (text content)\n",
    "#         with open(label_path, 'r', encoding='utf-8') as f:\n",
    "#             text = f.read().strip()\n",
    "\n",
    "#         # Create ground truth parse for Donut\n",
    "#         gt_parse = {\n",
    "#             \"text\": text  # e.g., \"danh mục đầu tư công năm 2021\"\n",
    "#         }\n",
    "\n",
    "#         # Create metadata entry\n",
    "#         metadata_entry = {\n",
    "#             \"file_name\": image_file,\n",
    "#             \"ground_truth\": json.dumps({\"gt_parse\": gt_parse})\n",
    "#         }\n",
    "\n",
    "#         # Copy image to appropriate split directory\n",
    "#         src_image_path = os.path.join(image_dir, image_file)\n",
    "#         if idx < train_end:\n",
    "#             split = 'train'\n",
    "#             train_metadata.append(metadata_entry)\n",
    "#             dst_image_path = os.path.join(output_dir, dataset_name, 'train', image_file)\n",
    "#         elif idx < val_end:\n",
    "#             split = 'validation'\n",
    "#             val_metadata.append(metadata_entry)\n",
    "#             dst_image_path = os.path.join(output_dir, dataset_name, 'validation', image_file)\n",
    "#         else:\n",
    "#             split = 'test'\n",
    "#             test_metadata.append(metadata_entry)\n",
    "#             dst_image_path = os.path.join(output_dir, dataset_name, 'test', image_file)\n",
    "\n",
    "#         # Copy image\n",
    "#         Image.open(src_image_path).save(dst_image_path)\n",
    "\n",
    "#     # Write metadata files\n",
    "#     for split, metadata in [('train', train_metadata), ('validation', val_metadata), ('test', test_metadata)]:\n",
    "#         if metadata:  # Only write if there's data\n",
    "#             metadata_path = os.path.join(output_dir, dataset_name, split, 'metadata.jsonl')\n",
    "#             with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "#                 for entry in metadata:\n",
    "#                     f.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Example usage\n",
    "#     data_dir = \"data_OCR\"  # Root directory containing final_data and labels\n",
    "#     output_dir = \"donut_data\"  # Where to save the Donut dataset\n",
    "#     dataset_name = \"my_donut_dataset\"\n",
    "    \n",
    "#     create_donut_dataset(data_dir, output_dir, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resume_from_checkpoint_path: None\n",
      "result_path: ./result\n",
      "\u001b[36mpretrained_model_name_or_path: naver-clova-ix/donut-base\n",
      "\u001b[0m\u001b[36mdataset_name_or_paths: [naver-clova-ix/cord-v2]\n",
      "\u001b[0msort_json_key: True\n",
      "train_batch_sizes: \n",
      "  - 2\n",
      "val_batch_sizes: \n",
      "  - 4\n",
      "input_size: \n",
      "  - 2560\n",
      "  - 1920\n",
      "max_length: 128\n",
      "align_long_axis: False\n",
      "num_nodes: 1\n",
      "seed: 2022\n",
      "lr: 3e-05\n",
      "warmup_steps: 10000\n",
      "num_training_samples_per_epoch: 39463\n",
      "max_epochs: 300\n",
      "max_steps: -1\n",
      "num_workers: 8\n",
      "val_check_interval: 1.0\n",
      "check_val_every_n_epoch: 1\n",
      "gradient_clip_val: 0.25\n",
      "verbose: True\n",
      "exp_name: train_docvqa\n",
      "exp_version: test_experiment\n",
      "Config is saved at result\\train_docvqa\\test_experiment\\config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 17:03:37.413846: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-15 17:03:39.584642: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "Seed set to 2022\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 534, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connection.py\", line 516, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py\", line 1395, in getresponse\n",
      "    response.begin()\n",
      "  File \"c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py\", line 325, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py\", line 286, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py\", line 1314, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py\", line 1166, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 841, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\util\\retry.py\", line 474, in increment\n",
      "    raise reraise(type(error), error, _stacktrace)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\util\\util.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 536, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 367, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1484, in _get_metadata_or_catch_error\n",
      "    metadata = get_hf_file_metadata(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1401, in get_hf_file_metadata\n",
      "    r = _request_wrapper(\n",
      "        ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 285, in _request_wrapper\n",
      "    response = _request_wrapper(\n",
      "               ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 308, in _request_wrapper\n",
      "    response = get_session().request(method=method, url=url, **params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py\", line 96, in send\n",
      "    return super().send(request, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\adapters.py\", line 713, in send\n",
      "    raise ReadTimeout(e, request=request)\n",
      "requests.exceptions.ReadTimeout: (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: a69e0e4b-d1b1-4e87-92de-9559106c86dd)')\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\hub.py\", line 424, in cached_files\n",
      "    hf_hub_download(\n",
      "  File \"c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 961, in hf_hub_download\n",
      "    return _hf_hub_download_to_cache_dir(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1068, in _hf_hub_download_to_cache_dir\n",
      "    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n",
      "  File \"c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1599, in _raise_on_head_call_error\n",
      "    raise LocalEntryNotFoundError(\n",
      "huggingface_hub.errors.LocalEntryNotFoundError: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Private\\Project\\kaggle\\LabAI\\Donut\\donut\\train.py\", line 176, in <module>\n",
      "    train(config)\n",
      "  File \"d:\\Private\\Project\\kaggle\\LabAI\\Donut\\donut\\train.py\", line 83, in train\n",
      "    model_module = DonutModelPLModule(config)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Private\\Project\\kaggle\\LabAI\\Donut\\donut\\lightning_module.py\", line 30, in __init__\n",
      "    self.model = DonutModel.from_pretrained(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Private\\Project\\kaggle\\LabAI\\Donut\\donut\\donut\\model.py\", line 597, in from_pretrained\n",
      "    model = super(DonutModel, cls).from_pretrained(pretrained_model_name_or_path, revision=\"official\", *model_args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 279, in _wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 4175, in from_pretrained\n",
      "    config, model_kwargs = cls.config_class.from_pretrained(\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\configuration_utils.py\", line 550, in from_pretrained\n",
      "    config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\configuration_utils.py\", line 590, in get_config_dict\n",
      "    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\configuration_utils.py\", line 649, in _get_config_dict\n",
      "    resolved_config_file = cached_file(\n",
      "                           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\hub.py\", line 266, in cached_file\n",
      "    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\hub.py\", line 491, in cached_files\n",
      "    raise OSError(\n",
      "OSError: We couldn't connect to 'https://huggingface.co' to load the files, and couldn't find them in the cached files.\n",
      "Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.\n"
     ]
    }
   ],
   "source": [
    "!python train.py --config config/train_docvqa.yaml --pretrained_model_name_or_path \"naver-clova-ix/donut-base\" --dataset_name_or_paths '[\"naver-clova-ix/cord-v2\"]' --exp_version \"test_experiment\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
