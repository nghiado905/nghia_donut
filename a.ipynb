{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# from collections import defaultdict\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def parse_cord_v2_json(json_path):\n",
    "#     with open(json_path, 'r', encoding='utf-8') as f:\n",
    "#         data = json.load(f)\n",
    "\n",
    "#     grouped = defaultdict(lambda: {\"count\": \"\", \"name\": \"\", \"price\": \"\"})\n",
    "\n",
    "#     total_price = \"\"\n",
    "    \n",
    "#     for line in data[\"valid_line\"]:\n",
    "#         category = line[\"category\"]\n",
    "#         group_id = line[\"group_id\"]\n",
    "#         words = \" \".join(w[\"text\"] for w in line[\"words\"])\n",
    "\n",
    "#         if category == \"menu.cnt\":\n",
    "#             grouped[group_id][\"count\"] = words\n",
    "#         elif category in [\"menu.nm\", \"menu.sub_nm\"]:\n",
    "#             grouped[group_id][\"name\"] += words + \" \"\n",
    "#         elif category == \"menu.price\":\n",
    "#             grouped[group_id][\"price\"] = words\n",
    "#         elif category == \"total.total_price\":\n",
    "#             total_price = words\n",
    "\n",
    "#     items = []\n",
    "#     for g in sorted(grouped.keys()):\n",
    "#         entry = grouped[g]\n",
    "#         name = entry[\"name\"].strip()\n",
    "#         if name and entry[\"price\"]:\n",
    "#             items.append({\n",
    "#                 \"name\": name,\n",
    "#                 \"count\": entry[\"count\"],\n",
    "#                 \"price\": entry[\"price\"]\n",
    "#             })\n",
    "\n",
    "#     result = {\n",
    "#         \"items\": items,\n",
    "#         \"total\": total_price\n",
    "#     }\n",
    "\n",
    "#     return result\n",
    "\n",
    "# def prepare_donut_cord_dataset(json_dir, image_dir, output_path):\n",
    "#     dataset = []\n",
    "\n",
    "#     json_files = [f for f in os.listdir(json_dir) if f.endswith(\".json\")]\n",
    "\n",
    "#     for json_file in tqdm(json_files, desc=\"Processing JSON files\"):\n",
    "#         json_path = os.path.join(json_dir, json_file)\n",
    "#         image_name = os.path.splitext(json_file)[0] + \".png\"  # n·∫øu l√† .jpg s·ª≠a ·ªü ƒë√¢y\n",
    "#         image_path = os.path.join(image_dir, image_name)\n",
    "\n",
    "#         if not os.path.exists(image_path):\n",
    "#             print(f\"Image not found for {json_file}, skipping...\")\n",
    "#             continue\n",
    "\n",
    "#         text_output = parse_cord_v2_json(json_path)\n",
    "        \n",
    "#         dataset.append({\n",
    "#             \"id\": json_file.replace(\".json\", \"\"),\n",
    "#             \"image_path\": image_path,\n",
    "#             \"text_input\": \"<s_cord-v2>\",  # prefix task\n",
    "#             \"text_output\": json.dumps(text_output, ensure_ascii=False)\n",
    "#         })\n",
    "\n",
    "#     with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#         json.dump(dataset, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "#     print(f\"‚úÖ Done! Saved {len(dataset)} samples to {output_path}\")\n",
    "\n",
    "# # C√°ch ch·∫°y\n",
    "# prepare_donut_cord_dataset(\n",
    "#     json_dir=\"data/json\",\n",
    "#     image_dir=\"data/image\",\n",
    "#     output_path=\"data/train.json\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# image_dir = \"data/images\"\n",
    "# annotation_dir = \"data/annotations\"\n",
    "\n",
    "# # Ki·ªÉm tra th∆∞ m·ª•c t·ªìn t·∫°i\n",
    "# print(\"Th∆∞ m·ª•c images t·ªìn t·∫°i:\", os.path.exists(image_dir))\n",
    "# print(\"Th∆∞ m·ª•c annotation t·ªìn t·∫°i:\", os.path.exists(annotation_dir))\n",
    "\n",
    "# # Li·ªát k√™ c√°c file trong th∆∞ m·ª•c\n",
    "# print(\"File trong images:\", os.listdir(image_dir))\n",
    "# print(\"File trong annotation:\", os.listdir(annotation_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# import shutil\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # ƒê·ªãnh nghƒ©a th∆∞ m·ª•c g·ªëc v√† t·ª∑ l·ªá chia d·ªØ li·ªáu\n",
    "# dataset_name = \"cord_dataset\"\n",
    "# image_dir = \"data/images\"  # Thay b·∫±ng ƒë∆∞·ªùng d·∫´n th·ª±c t·∫ø n·∫øu c·∫ßn\n",
    "# annotation_dir = \"data/annotations\"  # Thay b·∫±ng ƒë∆∞·ªùng d·∫´n th·ª±c t·∫ø n·∫øu c·∫ßn\n",
    "# train_ratio, val_ratio, test_ratio = 0.8, 0.1, 0.1\n",
    "\n",
    "# # Ki·ªÉm tra th∆∞ m·ª•c\n",
    "# if not os.path.exists(image_dir):\n",
    "#     raise FileNotFoundError(f\"Th∆∞ m·ª•c {image_dir} kh√¥ng t·ªìn t·∫°i\")\n",
    "# if not os.path.exists(annotation_dir):\n",
    "#     raise FileNotFoundError(f\"Th∆∞ m·ª•c {annotation_dir} kh√¥ng t·ªìn t·∫°i\")\n",
    "\n",
    "# # T·∫°o th∆∞ m·ª•c cho dataset\n",
    "# os.makedirs(dataset_name, exist_ok=True)\n",
    "# for split in [\"train\", \"validation\", \"test\"]:\n",
    "#     os.makedirs(os.path.join(dataset_name, split), exist_ok=True)\n",
    "\n",
    "# # H√†m x·ª≠ l√Ω file JSON ƒë·ªÉ t·∫°o ground truth\n",
    "# def process_cord_json(json_path):\n",
    "#     with open(json_path, 'r') as f:\n",
    "#         data = json.load(f)\n",
    "\n",
    "#     menu_items = []\n",
    "#     subtotal = None\n",
    "#     total_info = {}\n",
    "\n",
    "#     grouped_lines = {}\n",
    "#     for line in data[\"valid_line\"]:\n",
    "#         group_id = line[\"group_id\"]\n",
    "#         if group_id not in grouped_lines:\n",
    "#             grouped_lines[group_id] = []\n",
    "#         grouped_lines[group_id].append(line)\n",
    "\n",
    "#     for group_id, lines in grouped_lines.items():\n",
    "#         menu_item = {}\n",
    "#         for line in lines:\n",
    "#             category = line[\"category\"]\n",
    "#             text = \" \".join(word[\"text\"] for word in line[\"words\"])\n",
    "\n",
    "#             if category.startswith(\"menu\"):\n",
    "#                 if category == \"menu.nm\":\n",
    "#                     menu_item[\"name\"] = text\n",
    "#                 elif category == \"menu.cnt\":\n",
    "#                     menu_item[\"count\"] = text\n",
    "#                 elif category == \"menu.price\":\n",
    "#                     menu_item[\"price\"] = text\n",
    "#             elif category == \"sub_total.subtotal_price\":\n",
    "#                 subtotal = text\n",
    "#             elif category.startswith(\"total\"):\n",
    "#                 if category == \"total.total_price\":\n",
    "#                     total_info[\"total_price\"] = text\n",
    "#                 elif category == \"total.cashprice\":\n",
    "#                     total_info[\"cash\"] = text\n",
    "#                 elif category == \"total.changeprice\":\n",
    "#                     total_info[\"change\"] = text\n",
    "#                 elif category == \"total.menutype_cnt\":\n",
    "#                     total_info[\"menu_types\"] = text\n",
    "#                 elif category == \"total.menuqty_cnt\":\n",
    "#                     total_info[\"menu_quantity\"] = text\n",
    "\n",
    "#         if menu_item:\n",
    "#             menu_item.setdefault(\"count\", None)\n",
    "#             menu_item.setdefault(\"price\", None)\n",
    "#             menu_items.append(menu_item)\n",
    "\n",
    "#     ground_truth = {\n",
    "#         \"menu\": menu_items,\n",
    "#         \"subtotal\": subtotal,\n",
    "#         \"total\": total_info\n",
    "#     }\n",
    "#     return ground_truth\n",
    "\n",
    "# # Thu th·∫≠p t·∫•t c·∫£ c√°c file JSON v√† ·∫£nh\n",
    "# data = []\n",
    "# for json_file in os.listdir(annotation_dir):\n",
    "#     if json_file.endswith(\".json\"):\n",
    "#         image_file_base = json_file.replace(\".json\", \"\")\n",
    "#         image_path = None\n",
    "#         for ext in [\".jpg\", \".png\"]:  # H·ªó tr·ª£ nhi·ªÅu ƒë·ªãnh d·∫°ng ·∫£nh\n",
    "#             temp_path = os.path.join(image_dir, image_file_base + ext)\n",
    "#             if os.path.exists(temp_path):\n",
    "#                 image_path = temp_path\n",
    "#                 image_file = image_file_base + ext\n",
    "#                 break\n",
    "\n",
    "#         if image_path is None:\n",
    "#             print(f\"Kh√¥ng t√¨m th·∫•y ·∫£nh cho {json_file}\")\n",
    "#             continue\n",
    "\n",
    "#         json_path = os.path.join(annotation_dir, json_file)\n",
    "#         ground_truth = process_cord_json(json_path)\n",
    "#         data.append({\n",
    "#             \"image_file\": image_file,\n",
    "#             \"image_path\": image_path,\n",
    "#             \"ground_truth\": ground_truth\n",
    "#         })\n",
    "\n",
    "# # Ki·ªÉm tra xem c√≥ d·ªØ li·ªáu kh√¥ng\n",
    "# if not data:\n",
    "#     raise ValueError(\"Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu n√†o ƒë·ªÉ x·ª≠ l√Ω. Vui l√≤ng ki·ªÉm tra th∆∞ m·ª•c images v√† annotation.\")\n",
    "\n",
    "# # Chia d·ªØ li·ªáu th√†nh train, validation, test\n",
    "# train_data, temp_data = train_test_split(data, train_size=train_ratio, random_state=42)\n",
    "# val_data, test_data = train_test_split(temp_data, test_size=test_ratio/(test_ratio + val_ratio), random_state=42)\n",
    "\n",
    "# # H√†m l∆∞u d·ªØ li·ªáu v√†o th∆∞ m·ª•c v√† t·∫°o metadata.jsonl\n",
    "# def save_split(split_data, split_name):\n",
    "#     split_dir = os.path.join(dataset_name, split_name)\n",
    "#     metadata_path = os.path.join(split_dir, \"metadata.jsonl\")\n",
    "\n",
    "#     with open(metadata_path, \"w\") as f:\n",
    "#         for item in split_data:\n",
    "#             dest_image_path = os.path.join(split_dir, item[\"image_file\"])\n",
    "#             shutil.copy(item[\"image_path\"], dest_image_path)\n",
    "\n",
    "#             metadata = {\n",
    "#                 \"file_name\": item[\"image_file\"],\n",
    "#                 \"ground_truth\": json.dumps({\"gt_parse\": item[\"ground_truth\"]})\n",
    "#             }\n",
    "#             f.write(json.dumps(metadata) + \"\\n\")\n",
    "\n",
    "# # L∆∞u d·ªØ li·ªáu v√†o c√°c th∆∞ m·ª•c\n",
    "# save_split(train_data, \"train\")\n",
    "# save_split(val_data, \"validation\")\n",
    "# save_split(test_data, \"test\")\n",
    "\n",
    "# print(\"Dataset ƒë√£ ƒë∆∞·ª£c t·∫°o xong!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python test.py --dataset_name_or_path dataset --pretrained_model_name_or_path result_new/train_cord/test_experiment --save_path ./result/output.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gradio as gr\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "from donut import DonutModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_process(input_img):\n",
    "    global pretrained_model, task_prompt, task_name\n",
    "    input_img = Image.fromarray(input_img)\n",
    "    output = pretrained_model.inference(image=input_img, prompt=task_prompt)[\"predictions\"][0]\n",
    "    return output\n",
    "\n",
    "def demo_process_vqa(input_img, question):\n",
    "    global pretrained_model, task_prompt, task_name\n",
    "    input_img = Image.fromarray(input_img)\n",
    "    user_prompt = task_prompt.replace(\"{user_input}\", question)\n",
    "    output = pretrained_model.inference(input_img, prompt=user_prompt)[\"predictions\"][0]\n",
    "    return output\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--task\", type=str, default=\"cord-v2\")\n",
    "parser.add_argument(\"--pretrained_path\", type=str, default=\"naver-clova-ix/donut-base-finetuned-cord-v2\")\n",
    "args, left_argv = parser.parse_known_args()\n",
    "\n",
    "task_name = \"cord\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_prompt = f\"<s_{task_name}>\"\n",
    "\n",
    "pretrained_model = DonutModel.from_pretrained(r\"result_new/train_cord/test_experiment\")\n",
    "# pretrained_model = DonutModel.from_pretrained(r\"naver-clova-ix/donut-base-finetuned-cord-v2\")\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    pretrained_model.half()\n",
    "    device = torch.device(\"cuda\")\n",
    "    pretrained_model.to(device)\n",
    "else:\n",
    "    pretrained_model.encoder.to(torch.bfloat16)\n",
    "\n",
    "# pretrained_model.eval()\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=demo_process_vqa if task_name == \"docvqa\" else demo_process,\n",
    "    inputs=[\"image\", \"text\"] if task_name == \"docvqa\" else \"image\",\n",
    "    outputs=\"json\",\n",
    "    title=f\"Donut üç© demonstration for `{task_name}` task\",\n",
    ")\n",
    "demo.launch(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
