{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def merge_json_files(input_dir, output_file):\n",
    "    \"\"\"\n",
    "    Ghép tất cả các file JSON trong thư mục input_dir thành một file JSON duy nhất.\n",
    "    \n",
    "    Args:\n",
    "        input_dir (str): Đường dẫn đến thư mục chứa các file JSON\n",
    "        output_file (str): Đường dẫn đến file JSON đầu ra\n",
    "    \"\"\"\n",
    "    merged_data = []\n",
    "    \n",
    "    # Tìm tất cả các file JSON trong thư mục\n",
    "    json_files = glob.glob(os.path.join(input_dir, \"*.json\"))\n",
    "    \n",
    "    if not json_files:\n",
    "        print(\"Không tìm thấy file JSON nào trong thư mục:\", input_dir)\n",
    "        return merged_data  # Trả về danh sách rỗng thay vì None\n",
    "    \n",
    "    # Đọc và ghép từng file JSON\n",
    "    for json_file in json_files:\n",
    "        if os.path.basename(json_file) in [\"meta.json\", \"train.json\", \"test.json\", \"validation.json\"]:\n",
    "            continue\n",
    "        try:\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                print(f\"File {json_file} có kiểu dữ liệu: {type(data)}\")\n",
    "                # Chuẩn hóa dữ liệu: nếu không phải list, biến thành list\n",
    "                if not isinstance(data, list):\n",
    "                    data = [data]\n",
    "                merged_data.extend(data)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Lỗi khi đọc file {json_file}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi không xác định khi xử lý file {json_file}: {e}\")\n",
    "    \n",
    "    # Lưu dữ liệu ghép vào file đầu ra\n",
    "    try:\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(merged_data, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"Đã ghép thành công {len(json_files)} file JSON vào {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi ghi file đầu ra {output_file}: {e}\")\n",
    "    \n",
    "    return merged_data\n",
    "\n",
    "def split_dataset(data, input_dir, train_ratio=0.7, test_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Chia dữ liệu thành 3 tập: train, test, và validation.\n",
    "    \n",
    "    Args:\n",
    "        data (list): Dữ liệu cần chia\n",
    "        input_dir (str): Thư mục để lưu các file train.json, test.json, validation.json\n",
    "        train_ratio (float): Tỷ lệ tập train\n",
    "        test_ratio (float): Tỷ lệ tập test (validation_ratio sẽ là 1 - train_ratio - test_ratio)\n",
    "    \"\"\"\n",
    "    if not isinstance(data, list):\n",
    "        print(\"Dữ liệu đầu vào phải là một danh sách!\")\n",
    "        return\n",
    "    \n",
    "    if len(data) == 0:\n",
    "        print(\"Dữ liệu đầu vào rỗng, không thể chia tập!\")\n",
    "        return\n",
    "    \n",
    "    train_data, temp_data = train_test_split(data, train_size=train_ratio, random_state=42)\n",
    "    validation_ratio = (1 - train_ratio - test_ratio) / (1 - train_ratio)\n",
    "    test_data, validation_data = train_test_split(temp_data, train_size=(test_ratio / (test_ratio + (1 - train_ratio - test_ratio))), random_state=42)\n",
    "    \n",
    "    for dataset, filename in [(train_data, \"train.json\"), (test_data, \"test.json\"), (validation_data, \"validation.json\")]:\n",
    "        output_path = os.path.join(input_dir, filename)\n",
    "        try:\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(dataset, f, indent=4, ensure_ascii=False)\n",
    "            print(f\"Đã lưu {filename} với {len(dataset)} mẫu\")\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi khi ghi file {filename}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_directory = \"./json\"\n",
    "    output_dir = \"./dataset/\"\n",
    "    output_file = os.path.join(output_dir, \"meta.json\")\n",
    "    \n",
    "    merged_data = merge_json_files(input_directory, output_file)\n",
    "    print(f\"Kiểu dữ liệu của merged_data: {type(merged_data)}\")\n",
    "    \n",
    "    if merged_data is not None:\n",
    "        split_dataset(merged_data, output_dir, train_ratio=0.7, test_ratio=0.15)\n",
    "    else:\n",
    "        print(\"Không thể ghép dữ liệu, merged_data là None!\")                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def merge_json_files(input_dir, output_file):\n",
    "    \"\"\"\n",
    "    Ghép tất cả các file JSON trong thư mục input_dir thành một file JSON duy nhất.\n",
    "    \n",
    "    Args:\n",
    "        input_dir (str): Đường dẫn đến thư mục chứa các file JSON\n",
    "        output_file (str): Đường dẫn đến file JSON đầu ra\n",
    "    \"\"\"\n",
    "    merged_data = []\n",
    "    \n",
    "    # Tìm tất cả các file JSON trong thư mục\n",
    "    json_files = glob.glob(os.path.join(input_dir, \"*.json\"))\n",
    "    \n",
    "    if not json_files:\n",
    "        print(\"Không tìm thấy file JSON nào trong thư mục:\", input_dir)\n",
    "        return merged_data\n",
    "    \n",
    "    # Đọc và ghép từng file JSON\n",
    "    for json_file in json_files:\n",
    "        if os.path.basename(json_file) in [\"meta.json\", \"train.json\", \"test.json\", \"validation.json\"]:\n",
    "            continue\n",
    "        try:\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                print(f\"File {json_file} có kiểu dữ liệu: {type(data)}\")\n",
    "                if not isinstance(data, list):\n",
    "                    data = [data]\n",
    "                merged_data.extend(data)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Lỗi khi đọc file {json_file}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi không xác định khi xử lý file {json_file}: {e}\")\n",
    "    \n",
    "    # Lưu dữ liệu ghép vào file đầu ra\n",
    "    try:\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(merged_data, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"Đã ghép thành công {len(json_files)} file JSON vào {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi ghi file đầu ra {output_file}: {e}\")\n",
    "    \n",
    "    return merged_data\n",
    "\n",
    "def update_image_paths(meta_file, base_dir):\n",
    "    \"\"\"\n",
    "    Cập nhật đường dẫn tuyệt đối trong trường 'image' của meta.json thành đường dẫn tương đối.\n",
    "    \n",
    "    Args:\n",
    "        meta_file (str): Đường dẫn đến file meta.json\n",
    "        base_dir (str): Thư mục hiện tại để tính đường dẫn tương đối\n",
    "    Returns:\n",
    "        list: Dữ liệu đã cập nhật\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Đọc file meta.json\n",
    "        with open(meta_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Duyệt qua từng mục và cập nhật trường 'image'\n",
    "        for item in data:\n",
    "            if \"image\" in item:\n",
    "                abs_path = item[\"image\"]\n",
    "                rel_path = os.path.relpath(abs_path, base_dir)\n",
    "                item[\"image\"] = rel_path.replace(\"\\\\\", \"/\")\n",
    "            \n",
    "        # Lưu lại file meta.json\n",
    "        with open(meta_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"Đã cập nhật đường dẫn trong {meta_file}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi cập nhật đường dẫn trong {meta_file}: {e}\")\n",
    "        return []\n",
    "\n",
    "def split_dataset(data, input_dir, train_ratio=0.7, test_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Chia dữ liệu thành 3 tập: train, test, và validation.\n",
    "    \n",
    "    Args:\n",
    "        data (list): Dữ liệu cần chia\n",
    "        input_dir (str): Thư mục để lưu các file train.json, test.json, validation.json\n",
    "        train_ratio (float): Tỷ lệ tập train\n",
    "        test_ratio (float): Tỷ lệ tập test\n",
    "    \"\"\"\n",
    "    if not isinstance(data, list):\n",
    "        print(\"Dữ liệu đầu vào phải là một danh sách!\")\n",
    "        return\n",
    "    \n",
    "    if len(data) == 0:\n",
    "        print(\"Dữ liệu đầu vào rỗng, không thể chia tập!\")\n",
    "        return\n",
    "    \n",
    "    train_data, temp_data = train_test_split(data, train_size=train_ratio, random_state=42)\n",
    "    validation_ratio = (1 - train_ratio - test_ratio) / (1 - train_ratio)\n",
    "    test_data, validation_data = train_test_split(temp_data, train_size=(test_ratio / (test_ratio + (1 - train_ratio - test_ratio))), random_state=42)\n",
    "    \n",
    "    for dataset, filename in [(train_data, \"train.json\"), (test_data, \"test.json\"), (validation_data, \"validation.json\")]:\n",
    "        output_path = os.path.join(input_dir, filename)\n",
    "        try:\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(dataset, f, indent=4, ensure_ascii=False)\n",
    "            print(f\"Đã lưu {filename} với {len(dataset)} mẫu\")\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi khi ghi file {filename}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_directory = \"./json\"\n",
    "    output_dir = \"./dataset/\"\n",
    "    output_file = os.path.join(output_dir, \"meta.json\")\n",
    "    \n",
    "    # Ghép các file JSON\n",
    "    merged_data = merge_json_files(input_directory, output_file)\n",
    "    print(f\"Kiểu dữ liệu của merged_data: {type(merged_data)}\")\n",
    "    \n",
    "    if merged_data:\n",
    "        # Cập nhật đường dẫn và lấy dữ liệu đã sửa\n",
    "        updated_data = update_image_paths(output_file, output_dir)\n",
    "        if updated_data:\n",
    "            # Chia tập dữ liệu với dữ liệu đã cập nhật\n",
    "            split_dataset(updated_data, output_dir, train_ratio=0.7, test_ratio=0.15)\n",
    "        else:\n",
    "            print(\"Không thể lấy dữ liệu đã cập nhật, bỏ qua bước chia tập!\")\n",
    "    else:\n",
    "        print(\"Không thể ghép dữ liệu, merged_data rỗng!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def merge_json_files(input_dir, output_file):\n",
    "    \"\"\"\n",
    "    Ghép tất cả các file JSON trong thư mục input_dir thành một file JSON duy nhất.\n",
    "    \n",
    "    Args:\n",
    "        input_dir (str): Đường dẫn đến thư mục chứa các file JSON\n",
    "        output_file (str): Đường dẫn đến file JSON đầu ra\n",
    "    \"\"\"\n",
    "    merged_data = []\n",
    "    \n",
    "    json_files = glob.glob(os.path.join(input_dir, \"*.json\"))\n",
    "    \n",
    "    if not json_files:\n",
    "        print(\"Không tìm thấy file JSON nào trong thư mục:\", input_dir)\n",
    "        return merged_data\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        if os.path.basename(json_file) in [\"meta.json\", \"train.json\", \"test.json\", \"validation.json\"]:\n",
    "            continue\n",
    "        try:\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                print(f\"File {json_file} có kiểu dữ liệu: {type(data)}\")\n",
    "                if not isinstance(data, list):\n",
    "                    data = [data]\n",
    "                merged_data.extend(data)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Lỗi khi đọc file {json_file}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi không xác định khi xử lý file {json_file}: {e}\")\n",
    "    \n",
    "    try:\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(merged_data, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"Đã ghép thành công {len(json_files)} file JSON vào {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi ghi file đầu ra {output_file}: {e}\")\n",
    "    \n",
    "    return merged_data\n",
    "\n",
    "def update_image_paths(meta_file, base_dir):\n",
    "    \"\"\"\n",
    "    Cập nhật đường dẫn tuyệt đối trong trường 'image' thành đường dẫn tương đối.\n",
    "    \n",
    "    Args:\n",
    "        meta_file (str): Đường dẫn đến file meta.json\n",
    "        base_dir (str): Thư mục hiện tại để tính đường dẫn tương đối\n",
    "    Returns:\n",
    "        list: Dữ liệu đã cập nhật\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(meta_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        for item in data:\n",
    "            if \"image\" in item:\n",
    "                abs_path = item[\"image\"]\n",
    "                rel_path = os.path.relpath(abs_path, base_dir)\n",
    "                item[\"image\"] = rel_path.replace(\"\\\\\", \"/\")\n",
    "            \n",
    "        with open(meta_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"Đã cập nhật đường dẫn trong {meta_file}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi cập nhật đường dẫn trong {meta_file}: {e}\")\n",
    "        return []\n",
    "\n",
    "def split_dataset(data, output_dir, train_ratio=0.7, test_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Chia dữ liệu thành 3 tập: train, test, và validation, lưu dưới dạng metadata.jsonl.\n",
    "    \n",
    "    Args:\n",
    "        data (list): Dữ liệu cần chia\n",
    "        output_dir (str): Thư mục để lưu các thư mục train, test, validation\n",
    "        train_ratio (float): Tỷ lệ tập train\n",
    "        test_ratio (float): Tỷ lệ tập test\n",
    "    \"\"\"\n",
    "    if not isinstance(data, list):\n",
    "        print(\"Dữ liệu đầu vào phải là một danh sách!\")\n",
    "        return\n",
    "    \n",
    "    if len(data) == 0:\n",
    "        print(\"Dữ liệu đầu vào rỗng, không thể chia tập!\")\n",
    "        return\n",
    "    \n",
    "    # Chia dữ liệu\n",
    "    train_data, temp_data = train_test_split(data, train_size=train_ratio, random_state=42)\n",
    "    validation_ratio = (1 - train_ratio - test_ratio) / (1 - train_ratio)\n",
    "    test_data, validation_data = train_test_split(temp_data, train_size=(test_ratio / (test_ratio + (1 - train_ratio - test_ratio))), random_state=42)\n",
    "    \n",
    "    # Tạo thư mục và lưu dữ liệu dưới dạng JSONL\n",
    "    for dataset, split_name in [(train_data, \"train\"), (test_data, \"test\"), (validation_data, \"validation\")]:\n",
    "        split_dir = os.path.join(output_dir, split_name)\n",
    "        os.makedirs(split_dir, exist_ok=True)  # Tạo thư mục nếu chưa tồn tại\n",
    "        output_path = os.path.join(split_dir, \"metadata.jsonl\")\n",
    "        \n",
    "        try:\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                for item in dataset:\n",
    "                    # Ánh xạ dữ liệu sang định dạng JSONL\n",
    "                    jsonl_item = {\n",
    "                        \"file_name\": item.get(\"image\", \"\"),\n",
    "                        \"ground_truth\": json.dumps(item.get(\"ground_truth\", {\"gt_parse\": {}}), ensure_ascii=False)\n",
    "                    }\n",
    "                    f.write(json.dumps(jsonl_item, ensure_ascii=False) + \"\\n\")\n",
    "            print(f\"Đã lưu {split_name}/metadata.jsonl với {len(dataset)} mẫu\")\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi khi ghi file {output_path}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_directory = r\"D:\\Private\\Project\\kaggle\\LabAI\\Donut\\donut\\json\"\n",
    "    output_dir = r\"D:\\Private\\Project\\kaggle\\LabAI\\Donut\\donut\\dataset\"\n",
    "    output_file = os.path.join(output_dir, \"meta.json\")\n",
    "    \n",
    "    # Ghép các file JSON\n",
    "    merged_data = merge_json_files(input_directory, output_file)\n",
    "    print(f\"Kiểu dữ liệu của merged_data: {type(merged_data)}\")\n",
    "    \n",
    "    if merged_data:\n",
    "        # Cập nhật đường dẫn và lấy dữ liệu đã sửa\n",
    "        updated_data = update_image_paths(output_file, output_dir)\n",
    "        if updated_data:\n",
    "            # Chia tập dữ liệu với dữ liệu đã cập nhật\n",
    "            split_dataset(updated_data, output_dir, train_ratio=0.7, test_ratio=0.15)\n",
    "            # Xóa file meta.json\n",
    "            try:\n",
    "                os.remove(output_file)\n",
    "                print(f\"Đã xóa file {output_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Lỗi khi xóa file {output_file}: {e}\")\n",
    "        else:\n",
    "            print(\"Không thể lấy dữ liệu đã cập nhật, bỏ qua bước chia tập và xóa file!\")\n",
    "    else:\n",
    "        print(\"Không thể ghép dữ liệu, merged_data rỗng!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File D:/Private/Project/kaggle/LabAI/Donut/donut/json\\MBbank.json có kiểu dữ liệu: <class 'list'>\n",
      "File D:/Private/Project/kaggle/LabAI/Donut/donut/json\\SaigonCommercialBank.json có kiểu dữ liệu: <class 'list'>\n",
      "File D:/Private/Project/kaggle/LabAI/Donut/donut/json\\Techbank.json có kiểu dữ liệu: <class 'list'>\n",
      "File D:/Private/Project/kaggle/LabAI/Donut/donut/json\\Vietcombank.json có kiểu dữ liệu: <class 'list'>\n",
      "Đã ghép thành công 4 file JSON vào D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\meta.json\n",
      "Kiểu dữ liệu của merged_data: <class 'list'>\n",
      "Đã cập nhật đường dẫn trong D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\meta.json\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_34.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_MBbank_34.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_3.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_Techbank_3.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_51.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_Techbank_51.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_29.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_MBbank_29.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_41.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_MBbank_41.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_6.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_MBbank_6.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_1.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_Techbank_1.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Vietcombank_3.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_Vietcombank_3.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_40.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_MBbank_40.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_36.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_MBbank_36.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_17.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_MBbank_17.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_26.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_Techbank_26.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_35.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_MBbank_35.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_14.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_Techbank_14.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_8.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_MBbank_8.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_44.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_MBbank_44.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_13.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_Techbank_13.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_20.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_Techbank_20.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_28.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_MBbank_28.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_20.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_MBbank_20.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_38.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_Techbank_38.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Vietcombank_1.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_Vietcombank_1.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_26.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_MBbank_26.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_16.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_Techbank_16.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_50.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_MBbank_50.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_14.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_MBbank_14.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_25.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_MBbank_25.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_4.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_MBbank_4.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_18.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_MBbank_18.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_39.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_MBbank_39.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_9.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_MBbank_9.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_32.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_Techbank_32.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_7.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_MBbank_7.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_18.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_Techbank_18.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_42.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_Techbank_42.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_47.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_Techbank_47.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_2.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_Techbank_2.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_51.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_MBbank_51.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_21.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_Techbank_21.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_47.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_MBbank_47.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_45.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_Techbank_45.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_9.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_Techbank_9.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_52.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_Techbank_52.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_33.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_Techbank_33.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_42.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_MBbank_42.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_6.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_Techbank_6.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_49.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_MBbank_49.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_41.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_Techbank_41.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_5.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_Techbank_5.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_23.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_Techbank_23.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_33.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_MBbank_33.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_49.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_Techbank_49.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_7.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_Techbank_7.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_11.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_Techbank_11.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_39.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_Techbank_39.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_38.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_MBbank_38.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_30.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_MBbank_30.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_44.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_Techbank_44.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_2.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_MBbank_2.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_SaigonCommercialBank_2.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_SaigonCommercialBank_2.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_22.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_MBbank_22.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_3.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_MBbank_3.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_24.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_MBbank_24.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_35.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_Techbank_35.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_46.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_Techbank_46.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_22.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_Techbank_22.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_34.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_Techbank_34.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_30.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_Techbank_30.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_21.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_MBbank_21.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_8.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_Techbank_8.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_19.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_Techbank_19.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_15.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_MBbank_15.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_40.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_Techbank_40.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_SaigonCommercialBank_1.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_SaigonCommercialBank_1.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_50.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\train\\page_Techbank_50.jpg\n",
      "Đã lưu train/metadata.jsonl với 75 mẫu\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_5.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\test\\page_MBbank_5.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_45.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\test\\page_MBbank_45.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_31.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\test\\page_Techbank_31.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_48.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\test\\page_Techbank_48.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_27.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\test\\page_MBbank_27.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_36.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\test\\page_Techbank_36.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_12.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\test\\page_Techbank_12.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Vietcombank_2.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\test\\page_Vietcombank_2.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_32.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\test\\page_MBbank_32.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_31.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\test\\page_MBbank_31.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_13.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\test\\page_MBbank_13.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_10.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\test\\page_MBbank_10.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_46.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\test\\page_MBbank_46.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_29.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\test\\page_Techbank_29.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_19.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\test\\page_MBbank_19.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_16.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\test\\page_MBbank_16.jpg\n",
      "Đã lưu test/metadata.jsonl với 16 mẫu\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_37.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\validation\\page_Techbank_37.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_17.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\validation\\page_Techbank_17.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_48.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\validation\\page_MBbank_48.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_24.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\validation\\page_Techbank_24.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_43.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\validation\\page_Techbank_43.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_12.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\validation\\page_MBbank_12.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_43.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\validation\\page_MBbank_43.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_37.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\validation\\page_MBbank_37.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_1.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\validation\\page_MBbank_1.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_25.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\validation\\page_Techbank_25.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_10.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\validation\\page_Techbank_10.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_4.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\validation\\page_Techbank_4.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_15.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\validation\\page_Techbank_15.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_28.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\validation\\page_Techbank_28.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_Techbank_27.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\validation\\page_Techbank_27.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_23.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\validation\\page_MBbank_23.jpg\n",
      "Đã sao chép D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\\page_MBbank_11.jpg đến D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\validation\\page_MBbank_11.jpg\n",
      "Đã lưu validation/metadata.jsonl với 17 mẫu\n",
      "Đã xóa file D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\\meta.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def merge_json_files(input_dir, output_file):\n",
    "    \"\"\"\n",
    "    Ghép tất cả các file JSON trong thư mục input_dir thành một file JSON duy nhất.\n",
    "    \n",
    "    Args:\n",
    "        input_dir (str): Đường dẫn đến thư mục chứa các file JSON\n",
    "        output_file (str): Đường dẫn đến file JSON đầu ra\n",
    "    \"\"\"\n",
    "    merged_data = []\n",
    "    \n",
    "    json_files = glob.glob(os.path.join(input_dir, \"*.json\"))\n",
    "    \n",
    "    if not json_files:\n",
    "        print(\"Không tìm thấy file JSON nào trong thư mục:\", input_dir)\n",
    "        return merged_data\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        if os.path.basename(json_file) in [\"meta.json\", \"train.json\", \"test.json\", \"validation.json\"]:\n",
    "            continue\n",
    "        try:\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                print(f\"File {json_file} có kiểu dữ liệu: {type(data)}\")\n",
    "                if not isinstance(data, list):\n",
    "                    data = [data]\n",
    "                merged_data.extend(data)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Lỗi khi đọc file {json_file}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi không xác định khi xử lý file {json_file}: {e}\")\n",
    "    \n",
    "    try:\n",
    "        # Tạo thư mục chứa output_file nếu chưa tồn tại\n",
    "        output_dir = os.path.dirname(output_file)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(merged_data, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"Đã ghép thành công {len(json_files)} file JSON vào {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi ghi file đầu ra {output_file}: {e}\")\n",
    "    \n",
    "    return merged_data\n",
    "\n",
    "def update_image_paths(meta_file, images_dir):\n",
    "    \"\"\"\n",
    "    Cập nhật trường 'image' để chứa đường dẫn chính xác trong images_dir.\n",
    "    \n",
    "    Args:\n",
    "        meta_file (str): Đường dẫn đến file meta.json\n",
    "        images_dir (str): Thư mục chứa các file ảnh (/kaggle/working/nghia_donut/data/images/)\n",
    "    Returns:\n",
    "        list: Dữ liệu đã cập nhật\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(meta_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        for item in data:\n",
    "            if \"image\" in item:\n",
    "                # Chuẩn hóa đường dẫn và lấy tên file ảnh\n",
    "                image_path = item[\"image\"].replace(\"\\\\\", \"/\")\n",
    "                image_filename = os.path.basename(image_path)\n",
    "                # Tạo đường dẫn đầy đủ trong images_dir\n",
    "                full_image_path = os.path.join(images_dir, image_filename)\n",
    "                # Cập nhật trường image để chứa đường dẫn đầy đủ\n",
    "                item[\"image\"] = full_image_path\n",
    "        \n",
    "        with open(meta_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"Đã cập nhật đường dẫn trong {meta_file}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi cập nhật đường dẫn trong {meta_file}: {e}\")\n",
    "        return []\n",
    "\n",
    "def split_dataset(data, output_dir, train_ratio=0.7, test_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Chia dữ liệu thành 3 tập: train, test, và validation, lưu dưới dạng metadata.jsonl.\n",
    "    Sao chép các file ảnh vào thư mục tương ứng và cập nhật file_name.\n",
    "    \n",
    "    Args:\n",
    "        data (list): Dữ liệu cần chia\n",
    "        output_dir (str): Thư mục để lưu các thư mục train, test, validation\n",
    "        train_ratio (float): Tỷ lệ tập train\n",
    "        test_ratio (float): Tỷ lệ tập test\n",
    "    \"\"\"\n",
    "    if not isinstance(data, list):\n",
    "        print(\"Dữ liệu đầu vào phải là một danh sách!\")\n",
    "        return\n",
    "    \n",
    "    if len(data) == 0:\n",
    "        print(\"Dữ liệu đầu vào rỗng, không thể chia tập!\")\n",
    "        return\n",
    "    \n",
    "    # Chia dữ liệu\n",
    "    train_data, temp_data = train_test_split(data, train_size=train_ratio, random_state=42)\n",
    "    validation_ratio = (1 - train_ratio - test_ratio) / (1 - train_ratio)\n",
    "    test_data, validation_data = train_test_split(temp_data, train_size=(test_ratio / (test_ratio + (1 - train_ratio - test_ratio))), random_state=42)\n",
    "    \n",
    "    # Tạo thư mục và lưu dữ liệu dưới dạng JSONL\n",
    "    for dataset, split_name in [(train_data, \"train\"), (test_data, \"test\"), (validation_data, \"validation\")]:\n",
    "        split_dir = os.path.join(output_dir, split_name)\n",
    "        os.makedirs(split_dir, exist_ok=True)  # Tạo thư mục nếu chưa tồn tại\n",
    "        output_path = os.path.join(split_dir, \"metadata.jsonl\")\n",
    "        \n",
    "        try:\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                for item in dataset:\n",
    "                    # Lấy đường dẫn ảnh (đã được cập nhật trong update_image_paths)\n",
    "                    image_path = item.get(\"image\", \"\")\n",
    "                    if not image_path or not os.path.exists(image_path):\n",
    "                        print(f\"Cảnh báo: File ảnh {image_path} không tồn tại, bỏ qua mục này\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Lấy tên file ảnh\n",
    "                    image_filename = os.path.basename(image_path)\n",
    "                    # Đường dẫn đích để sao chép file ảnh\n",
    "                    dest_image_path = os.path.join(split_dir, image_filename)\n",
    "                    \n",
    "                    # Sao chép file ảnh vào thư mục split\n",
    "                    try:\n",
    "                        shutil.copy2(image_path, dest_image_path)\n",
    "                        print(f\"Đã sao chép {image_path} đến {dest_image_path}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Lỗi khi sao chép file ảnh {image_path}: {e}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Ánh xạ dữ liệu sang định dạng JSONL\n",
    "                    jsonl_item = {\n",
    "                        \"file_name\": image_filename,  # Chỉ sử dụng tên file\n",
    "                        \"ground_truth\": json.dumps(item.get(\"ground_truth\", {\"gt_parse\": {}}), ensure_ascii=False)\n",
    "                    }\n",
    "                    f.write(json.dumps(jsonl_item, ensure_ascii=False) + \"\\n\")\n",
    "            print(f\"Đã lưu {split_name}/metadata.jsonl với {len(dataset)} mẫu\")\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi khi ghi file {output_path}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Đường dẫn đến thư mục chứa các file JSON\n",
    "    input_directory = \"D:/Private/Project/kaggle/LabAI/Donut/donut/json\"\n",
    "    # Đường dẫn đến thư mục dataset\n",
    "    output_dir = \"D:/Private/Project/kaggle/LabAI/Donut/donut/dataset\"\n",
    "    # Đường dẫn đến thư mục chứa file ảnh gốc\n",
    "    images_dir = \"D:/Private/Project/kaggle/LabAI/Donut/donut/data/images\"\n",
    "    # Đường dẫn đến file meta.json\n",
    "    output_file = os.path.join(output_dir, \"meta.json\")\n",
    "    \n",
    "    # Ghép các file JSON\n",
    "    merged_data = merge_json_files(input_directory, output_file)\n",
    "    print(f\"Kiểu dữ liệu của merged_data: {type(merged_data)}\")\n",
    "    \n",
    "    if merged_data:\n",
    "        # Cập nhật đường dẫn và lấy dữ liệu đã sửa\n",
    "        updated_data = update_image_paths(output_file, images_dir)\n",
    "        if updated_data:\n",
    "            # Chia tập dữ liệu với dữ liệu đã cập nhật\n",
    "            split_dataset(updated_data, output_dir, train_ratio=0.7, test_ratio=0.15)\n",
    "            # Xóa file meta.json\n",
    "            try:\n",
    "                os.remove(output_file)\n",
    "                print(f\"Đã xóa file {output_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Lỗi khi xóa file {output_file}: {e}\")\n",
    "        else:\n",
    "            print(\"Không thể lấy dữ liệu đã cập nhật, bỏ qua bước chia tập và xóa file!\")\n",
    "    else:\n",
    "        print(\"Không thể ghép dữ liệu, merged_data rỗng!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 16, Tree Edit Distance (TED) based accuracy score: 0.019463551499399913, F1 accuracy score: 0.005638340716874748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
      "c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3638.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "\n",
      "Downloading data:   0%|          | 0/76 [00:00<?, ?files/s]\n",
      "Downloading data: 100%|██████████| 76/76 [00:00<00:00, 76314.84files/s]\n",
      "\n",
      "Downloading data:   0%|          | 0/18 [00:00<?, ?files/s]\n",
      "Downloading data: 100%|██████████| 18/18 [00:00<00:00, 18014.19files/s]\n",
      "\n",
      "Downloading data:   0%|          | 0/17 [00:00<?, ?files/s]\n",
      "Downloading data: 100%|██████████| 17/17 [00:00<?, ?files/s]\n",
      "\n",
      "Generating train split: 0 examples [00:00, ? examples/s]\n",
      "Generating train split: 75 examples [00:00, 4044.39 examples/s]\n",
      "\n",
      "Generating validation split: 0 examples [00:00, ? examples/s]\n",
      "Generating validation split: 17 examples [00:00, 1353.77 examples/s]\n",
      "\n",
      "Generating test split: 0 examples [00:00, ? examples/s]\n",
      "Generating test split: 16 examples [00:00, 1260.52 examples/s]\n",
      "\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\n",
      "  6%|▋         | 1/16 [00:09<02:17,  9.16s/it]\n",
      " 12%|█▎        | 2/16 [00:13<01:28,  6.33s/it]\n",
      " 19%|█▉        | 3/16 [00:15<00:57,  4.39s/it]\n",
      " 25%|██▌       | 4/16 [00:17<00:40,  3.34s/it]\n",
      " 31%|███▏      | 5/16 [00:18<00:30,  2.74s/it]\n",
      " 38%|███▊      | 6/16 [00:20<00:23,  2.35s/it]\n",
      " 44%|████▍     | 7/16 [00:22<00:18,  2.11s/it]\n",
      " 50%|█████     | 8/16 [00:23<00:15,  1.96s/it]\n",
      " 56%|█████▋    | 9/16 [00:25<00:13,  1.91s/it]\n",
      " 62%|██████▎   | 10/16 [00:27<00:11,  1.93s/it]\n",
      " 69%|██████▉   | 11/16 [00:29<00:09,  1.96s/it]\n",
      " 75%|███████▌  | 12/16 [00:31<00:07,  1.93s/it]\n",
      " 81%|████████▏ | 13/16 [00:33<00:05,  1.86s/it]\n",
      " 88%|████████▊ | 14/16 [00:35<00:03,  1.85s/it]\n",
      " 94%|█████████▍| 15/16 [00:36<00:01,  1.85s/it]\n",
      "100%|██████████| 16/16 [00:38<00:00,  1.92s/it]\n",
      "100%|██████████| 16/16 [00:38<00:00,  2.44s/it]\n"
     ]
    }
   ],
   "source": [
    "!python test.py --dataset_name_or_path ./dataset --pretrained_model_name_or_path ./result_new/train_cord/test_experiment --save_path ./result/output.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load image processor for 'result_new/config/my_experiment'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'result_new/config/my_experiment' is the correct path to a directory containing a preprocessor_config.json file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\image_processing_utils.py:257\u001b[0m, in \u001b[0;36mImageProcessingMixin.get_image_processor_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[1;32m--> 257\u001b[0m     resolved_image_processor_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_processor_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\hub.py:409\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    408\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 409\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n",
      "File \u001b[1;32mc:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:154\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[1;34m(repo_id)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m     )\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n",
      "\u001b[1;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'result_new/config/my_experiment'. Use `repo_type` argument if needed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Tải processor và mô hình\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m processor \u001b[38;5;241m=\u001b[39m \u001b[43mDonutProcessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresult_new/config/my_experiment\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m VisionEncoderDecoderModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult_new/config/my_experiment\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Tải và tiền xử lý ảnh\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\processing_utils.py:183\u001b[0m, in \u001b[0;36mProcessorMixin.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pretrained\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    155\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03m    Instantiate a processor associated with a pretrained model.\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;124;03m            [`~tokenization_utils_base.PreTrainedTokenizer.from_pretrained`].\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 183\u001b[0m     args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_arguments_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[1;32mc:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\processing_utils.py:227\u001b[0m, in \u001b[0;36mProcessorMixin._get_arguments_from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    225\u001b[0m         attribute_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(transformers_module, class_name)\n\u001b[1;32m--> 227\u001b[0m     args\u001b[38;5;241m.\u001b[39mappend(\u001b[43mattribute_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m args\n",
      "File \u001b[1;32mc:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\auto\\image_processing_auto.py:297\u001b[0m, in \u001b[0;36mAutoImageProcessor.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    295\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_auto\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 297\u001b[0m config_dict, _ \u001b[38;5;241m=\u001b[39m \u001b[43mImageProcessingMixin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image_processor_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m image_processor_class \u001b[38;5;241m=\u001b[39m config_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_processor_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    299\u001b[0m image_processor_auto_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\image_processing_utils.py:275\u001b[0m, in \u001b[0;36mImageProcessingMixin.get_image_processor_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m         \u001b[38;5;66;03m# For any other exception, we throw a generic error.\u001b[39;00m\n\u001b[1;32m--> 275\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    276\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load image processor for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    277\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m it from \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m same name. Otherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m directory containing a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mIMAGE_PROCESSOR_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m file\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         )\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;66;03m# Load image_processor dict\u001b[39;00m\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(resolved_image_processor_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m reader:\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load image processor for 'result_new/config/my_experiment'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'result_new/config/my_experiment' is the correct path to a directory containing a preprocessor_config.json file"
     ]
    }
   ],
   "source": [
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Tải processor và mô hình\n",
    "processor = DonutProcessor.from_pretrained(\"result_new/config/my_experiment\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"result_new/config/my_experiment\")\n",
    "\n",
    "# Tải và tiền xử lý ảnh\n",
    "image = Image.open(r\"D:\\Private\\Project\\kaggle\\LabAI\\Donut\\donut\\dataset\\train\\page_MBbank_2.jpg\").convert(\"RGB\")\n",
    "pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "# Chuyển sang GPU nếu có\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "pixel_values = pixel_values.to(device)\n",
    "\n",
    "# Tạo dự đoán\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    task_prompt = \"<s_dataset>\"  # Điều chỉnh nếu cần\n",
    "    decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids.to(device)\n",
    "    outputs = model.generate(\n",
    "        pixel_values,\n",
    "        decoder_input_ids=decoder_input_ids,\n",
    "        max_length=128,\n",
    "        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "        eos_token_id=processor.tokenizer.eos_token_id,\n",
    "        use_cache=True,\n",
    "        num_beams=1,\n",
    "        bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "        return_dict_in_generate=True,\n",
    "    )\n",
    "\n",
    "# Giải mã đầu ra\n",
    "decoded_output = processor.tokenizer.batch_decode(outputs.sequences)[0]\n",
    "print(\"Dự đoán:\", decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
